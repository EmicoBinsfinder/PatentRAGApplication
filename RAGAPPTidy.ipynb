{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "import os\n",
    "import base64\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "import uuid\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import openai\n",
    "import gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.getcwd()\n",
    "output_path = 'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "openai.api_key = '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'\n",
    "\n",
    "# OpenAI Model\n",
    "\n",
    "chain_gpt_35 = ChatOpenAI(model=\"gpt-4-0125-preview\", max_tokens=1024, api_key=OPENAI_API_KEY)\n",
    "chain_gpt_4_vision = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to encode images for use with GPT-4 Vision Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to create summaries from PDF Text, Tables and Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text summaries\n",
    "def summarize_text(text_element):\n",
    "    prompt = f\"Summarize in detail the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for table summaries\n",
    "def summarize_table(table_element):\n",
    "    prompt = f\"Summarize in detail the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for image summaries\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are a bot that is good at analyzing images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the contents of this image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = chain_gpt_4_vision.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define retreival algorithm and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Initialize the retriever\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to add documents to retrieval algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add documents to the retriever\n",
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    id_key = \"doc_id\"\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding PDF information into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patents = ['us10745814', 'us2021229059']\n",
    "\n",
    "for patent in Patents:\n",
    "\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=os.path.join(input_path, f\"{patent}.pdf\"),\n",
    "        extract_images_in_pdf=True, # Get images \n",
    "        infer_table_structure=True, # Get tables \n",
    "        chunking_strategy=\"by_title\", # Preserves sections (by headings and subheadings, as well as over pages where possible)\n",
    "        max_characters=4000, # Define chunk size\n",
    "        new_after_n_chars=3800, # Soft max chunk size \n",
    "        extract_image_block_types=['Image'],\n",
    "        combine_text_under_n_chars=2000, # Min length of chunk size\n",
    "        extract_image_block_output_dir=f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images') #Directory to store images\n",
    "\n",
    "\n",
    "    # Save table and text elements to list\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    image_elements = []\n",
    "\n",
    "    for element in raw_pdf_elements:\n",
    "        if 'CompositeElement' in str(type(element)):\n",
    "            text_elements.append(element)\n",
    "        elif 'Table' in str(type(element)):\n",
    "            table_elements.append(element)\n",
    "\n",
    "    table_elements = [i.text for i in table_elements]\n",
    "    text_elements = [i.text for i in text_elements]\n",
    "\n",
    "    # Save image elements to list, converting to format compatible with Vision transformer model\n",
    "\n",
    "    for image_file in os.listdir(f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images'):\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images', image_file)\n",
    "            encoded_image = encode_image(image_path)\n",
    "            image_elements.append(encoded_image)\n",
    "\n",
    "    # Generate Table summaries\n",
    "    table_summaries = []\n",
    "    \n",
    "    for i, te in enumerate(table_elements):\n",
    "        summary = summarize_table(te)\n",
    "        table_summaries.append(summary)\n",
    "        print(f\"{i + 1}th element of tables processed.\")\n",
    "\n",
    "    # Generate Text element summaries\n",
    "    text_summaries = []\n",
    "    for i, te in enumerate(text_elements):\n",
    "        summary = summarize_text(te)\n",
    "        text_summaries.append(summary)\n",
    "        print(f\"{i + 1}th element of texts processed.\")\n",
    "\n",
    "    # Generate summaries of images\n",
    "    image_summaries = []\n",
    "    for i, ie in enumerate(image_elements):\n",
    "        try:\n",
    "            summary = summarize_image(ie)\n",
    "            image_summaries.append(summary)\n",
    "        except:\n",
    "            print(f'{ie} could not be summarised, could be too large')\n",
    "        print(f\"{i + 1}th element of images processed.\")\n",
    "\n",
    "    # Add text, table and image summaries to vector database\n",
    "\n",
    "    add_documents_to_retriever(text_summaries, text_elements)\n",
    "    add_documents_to_retriever(table_summaries, table_elements)\n",
    "    add_documents_to_retriever(image_summaries, image_summaries) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002916281B820> docstore=<langchain.storage.in_memory.InMemoryBaseStore object at 0x000002916281BF10>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define prompt template\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text, images and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# response = chain.invoke('What are some methods for electrically controlled nucleotide chain synthesis?')\n",
    "\n",
    "print(len(retriever.get_relevant_documents('What are some methods for electrically controlled nucleotide chain synthesis?')))\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\blocks.py:530: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-65ca9568-39de7eac16a7165b5af35ac5;d7884aa2-9dfd-48fe-a02b-63358b5c5366)\n",
      "\n",
      "Sorry, we can't find the page you are looking for.\n",
      "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://b99a31e870e0963fb5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b99a31e870e0963fb5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1591, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\eeo21\\AppData\\Local\\Temp\\ipykernel_8908\\3233520793.py\", line 8, in chatbot\n",
      "    chat = openai.ChatCompletion.create(\n",
      "  File \"c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\openai\\lib\\_old_api.py\", line 39, in __call__\n",
      "    raise APIRemovedInV1(symbol=self._symbol)\n",
      "openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and kind AI Assistant.\"},\n",
    "]\n",
    "\n",
    "def chatbot(input):\n",
    "    if input:\n",
    "        messages.append({\"role\": \"user\", \"content\": input})\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "        )\n",
    "        reply = chat.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "inputs = gradio.Textbox(lines=7, label=\"Chat with AI\")\n",
    "outputs = gradio.Textbox(label=\"Reply\")\n",
    "\n",
    "gradio.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"Your First AI ChatBot\",\n",
    "             description=\"Ask anything you want\",\n",
    "             theme=\"compact\").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
