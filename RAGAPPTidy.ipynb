{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "import os\n",
    "import base64\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "import uuid\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import openai\n",
    "import gradio\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.getcwd()\n",
    "output_path = 'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "openai.api_key = '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'\n",
    "\n",
    "# OpenAI Model\n",
    "\n",
    "chain_gpt_35 = ChatOpenAI(model=\"gpt-4-0125-preview\", max_tokens=1024, api_key=OPENAI_API_KEY)\n",
    "chain_gpt_4_vision = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to encode images for use with GPT-4 Vision Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to create summaries from PDF Text, Tables and Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text summaries\n",
    "def summarize_text(text_element):\n",
    "    prompt = f\"Summarize in detail the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for table summaries\n",
    "def summarize_table(table_element):\n",
    "    prompt = f\"Summarize in detail the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for image summaries\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are a bot that is good at analyzing images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the contents of this image from a patent application\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = chain_gpt_4_vision.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define retreival algorithm and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Initialize the retriever\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to add documents to retrieval algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add documents to the retriever\n",
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    id_key = \"doc_id\"\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding PDF information into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th element of images processed.\n",
      "2th element of images processed.\n",
      "3th element of images processed.\n",
      "4th element of images processed.\n",
      "5th element of images processed.\n",
      "6th element of images processed.\n",
      "1th element of images processed.\n",
      "2th element of images processed.\n",
      "3th element of images processed.\n",
      "4th element of images processed.\n",
      "5th element of images processed.\n",
      "6th element of images processed.\n",
      "7th element of images processed.\n",
      "8th element of images processed.\n",
      "9th element of images processed.\n",
      "10th element of images processed.\n",
      "11th element of images processed.\n",
      "1th element of images processed.\n",
      "2th element of images processed.\n",
      "3th element of images processed.\n",
      "4th element of images processed.\n",
      "5th element of images processed.\n",
      "6th element of images processed.\n",
      "7th element of images processed.\n",
      "8th element of images processed.\n",
      "9th element of images processed.\n",
      "10th element of images processed.\n",
      "11th element of images processed.\n",
      "12th element of images processed.\n",
      "13th element of images processed.\n",
      "14th element of images processed.\n",
      "15th element of images processed.\n",
      "16th element of images processed.\n",
      "17th element of images processed.\n",
      "18th element of images processed.\n",
      "19th element of images processed.\n",
      "20th element of images processed.\n",
      "21th element of images processed.\n",
      "22th element of images processed.\n",
      "23th element of images processed.\n",
      "24th element of images processed.\n",
      "25th element of images processed.\n",
      "26th element of images processed.\n",
      "27th element of images processed.\n",
      "28th element of images processed.\n",
      "29th element of images processed.\n",
      "30th element of images processed.\n",
      "1th element of images processed.\n",
      "2th element of images processed.\n",
      "3th element of images processed.\n",
      "4th element of images processed.\n",
      "5th element of images processed.\n",
      "6th element of images processed.\n",
      "7th element of images processed.\n",
      "8th element of images processed.\n",
      "9th element of images processed.\n"
     ]
    }
   ],
   "source": [
    "Patents = ['US10196086', 'US20180193694A1', 'us10745814', 'us2021229059']\n",
    "\n",
    "for patent in Patents:\n",
    "\n",
    "    # raw_pdf_elements = partition_pdf(\n",
    "    #     filename=os.path.join(input_path, f\"{patent}.pdf\"),\n",
    "    #     extract_images_in_pdf=True, # Get images \n",
    "    #     infer_table_structure=True, # Get tables \n",
    "    #     chunking_strategy=\"by_title\", # Preserves sections (by headings and subheadings, as well as over pages where possible)\n",
    "    #     max_characters=3000, # Define chunk size\n",
    "    #     new_after_n_chars=1900, # Soft max chunk size \n",
    "    #     extract_image_block_types=['Image'],\n",
    "    #     combine_text_under_n_chars=1000, # Min length of chunk size\n",
    "    #     extract_image_block_output_dir=f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images') #Directory to store images\n",
    "\n",
    "\n",
    "    # Save table and text elements to list\n",
    "    # text_elements = []\n",
    "    # table_elements = []\n",
    "    image_elements = []\n",
    "\n",
    "    # for element in raw_pdf_elements:\n",
    "    #     if 'CompositeElement' in str(type(element)):\n",
    "    #         text_elements.append(element)\n",
    "    #     elif 'Table' in str(type(element)):\n",
    "    #         table_elements.append(element)\n",
    "    \n",
    "    patent_id = f'###_### {patent}' \n",
    "\n",
    "    # table_elements = [i.text for i in table_elements]\n",
    "    # table_elements = [''.join([i, patent_id]) for i in table_elements]\n",
    "\n",
    "    # text_elements = [i.text for i in text_elements]\n",
    "    # text_elements = [''.join([i, patent_id]) for i in text_elements]\n",
    "\n",
    "    # Save image elements to list, converting to format compatible with Vision transformer model\n",
    "\n",
    "    for image_file in os.listdir(f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images'):\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images', image_file)\n",
    "            encoded_image = encode_image(image_path)\n",
    "            image_elements.append(encoded_image)\n",
    "\n",
    "    # # Generate Table summaries\n",
    "    # table_summaries = []\n",
    "    \n",
    "    # for i, te in enumerate(table_elements):\n",
    "    #     summary = summarize_table(te)\n",
    "    #     summary = ''.join([summary, patent_id])\n",
    "    #     table_summaries.append(summary)\n",
    "    #     print(f\"{i + 1}th element of tables processed.\")\n",
    "\n",
    "    # # Generate Text element summaries\n",
    "    # text_summaries = []\n",
    "    # for i, te in enumerate(text_elements):\n",
    "    #     summary = summarize_text(te)\n",
    "    #     summary = ''.join([summary, patent_id])\n",
    "    #     text_summaries.append(summary)\n",
    "    #     print(f\"{i + 1}th element of texts processed.\")\n",
    "\n",
    "    # Generate summaries of images\n",
    "    image_summaries = []\n",
    "    for i, ie in enumerate(image_elements):\n",
    "        try:\n",
    "            summary = summarize_image(ie)\n",
    "            summary = ''.join([summary, patent_id])\n",
    "            image_summaries.append(summary)\n",
    "        except:\n",
    "            print(f'{ie} could not be summarised, could be too large')\n",
    "        print(f\"{i + 1}th element of images processed.\")\n",
    "\n",
    "    # Add text, table and image summaries to vector database\n",
    "    # add_documents_to_retriever(text_summaries, text_elements)\n",
    "    # add_documents_to_retriever(table_summaries, table_elements)\n",
    "    add_documents_to_retriever(image_summaries, image_summaries) \n",
    "\n",
    "    # Save pdf elements to databases \n",
    "\n",
    "    # texts =list(zip(text_summaries, text_elements))\n",
    "    # tables = list(zip(table_summaries, table_elements))\n",
    "    images = list(zip(image_summaries, image_summaries))\n",
    "\n",
    "    components = [images]\n",
    "    \n",
    "    PDFdictionary = {}\n",
    "    for component in components:\n",
    "        for pair in component:\n",
    "            PDFdictionary[pair[0]] = [pair[1]]\n",
    "    \n",
    "    with open(f\"{patent}_images.json\", \"w\") as outfile: \n",
    "        json.dump(PDFdictionary, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define inference model and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define prompt template\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text, images and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "Aim = 'Quality' #Change to slow for faster response, perhaps with lower quality\n",
    "\n",
    "if Aim == 'Quality':\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "elif Aim == 'Fast':\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eeo21\\Anaconda3\\lib\\site-packages\\gradio\\blocks.py:530: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-65cb6b19-1c4db5907ed98efb62244661;f7b49c70-5e4a-4de1-8236-618d2f0bf1d8)\n",
      "\n",
      "Sorry, we can't find the page you are looking for.\n",
      "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://6bac85bbb645fd31fb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6bac85bbb645fd31fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images in US patent US10196086 are not directly described in the provided text. The text references a different patent application, US20180193694A1, and describes images from this application instead. Therefore, based on the provided context, I cannot accurately describe the images in US patent US10196086.\n",
      "The images in US patent US20180193694A1 depict various figures related to the patent application, including an oval-shaped object with a dotted pattern, possibly indicating holes, indentations, or raised points, and a rectangular area within the oval shape that might represent a different section or component. The images also include reference numbers like 410, 414, and 412, which are typically used to describe parts of the design in detail within the patent document. Additionally, there are figures that seem to include text and possibly other diagrams or illustrations related to the patent, but without specific details on their content, it's challenging to determine their exact nature or function. The patent appears to cover a design or invention involving detailed components, as indicated by the various figures and reference numbers provided.\n",
      "Based on the context provided, there is no direct information or description of the images in US patent US10745814. The text provided discusses images from patent applications US20180193694A1 and US2021229059, but does not mention or describe images from US10745814. Therefore, without additional context or descriptions specific to US10745814, it's not possible to accurately describe what the images in that patent show.\n",
      "The images in US patent US2021229059 show an oval-shaped object with a dotted pattern throughout, featuring a clearly defined border around the edge and a rectangular area within the object, possibly indicating a different section or component. The dots might represent holes, indentations, or raised points. There are reference numbers (like 410, 414, and 412) that typically correspond to a detailed description in the patent document, explaining what each part represents. This description suggests that the images are part of a design or invention detailed in the patent, but without additional context, the exact nature or function of this design is not specified.\n",
      "I'm sorry, but I cannot provide information or descriptions of images in US patent US10196086 based on the context provided, as it does not include any specific details or descriptions related to that patent. My responses are generated based on the information available in the provided context, and the details given pertain to patent application US20180193694A1 and a general description of another patent figure, not US10196086. To learn about the images in US patent US10196086, I recommend consulting the official United States Patent and Trademark Office (USPTO) database or other reliable sources that offer access to patent documents and images.\n"
     ]
    }
   ],
   "source": [
    "def chatbot(input):\n",
    "    response = chain.invoke(f'{input}')\n",
    "\n",
    "    contextlist = retriever.get_relevant_documents(f'{input}')\n",
    "\n",
    "    #Remove any duplicated contexts\n",
    "    contextlist = list(dict.fromkeys(contextlist))\n",
    "\n",
    "    #Get ID of patent that context was retreived from \n",
    "    for x in contextlist:\n",
    "        comps = x.split('###_###')\n",
    "        context = comps[0]\n",
    "        ID = comps[1]\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    # Prettify response\n",
    "    response = response.replace('**', '')\n",
    "    response = '\\n'.join([response, f\"{'#'*120} \\n\"])\n",
    "    response = '\\n'.join([response, f\"CONTEXT USED TO GENERATE RESPONSE:\\n {context}\"])\n",
    "    response = '\\n'.join([response, f\"{'#'*120} \\n\"])\n",
    "    response = '\\n'.join([response, f\"CONTEXT FOUND IN PATENT NO. {ID}\"])\n",
    "   \n",
    "    return response\n",
    "\n",
    "inputs = gradio.Textbox(lines=7, label=\"Ask question based on facts from a patent\")\n",
    "outputs = gradio.Textbox(label=\"Reply\")\n",
    "\n",
    "gradio.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"Patent RAG Prototype\",\n",
    "             theme=\"compact\").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
