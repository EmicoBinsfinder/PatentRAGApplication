{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "import os\n",
    "import base64\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "import uuid\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import openai\n",
    "import gradio\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.getcwd()\n",
    "output_path = 'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = ''\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "openai.api_key = ''\n",
    "\n",
    "# OpenAI Model\n",
    "\n",
    "chain_gpt_35 = ChatOpenAI(model=\"gpt-4-0125-preview\", max_tokens=1024, api_key=OPENAI_API_KEY)\n",
    "chain_gpt_4_vision = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to encode images for use with GPT-4 Vision Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to create summaries from PDF Text, Tables and Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text summaries\n",
    "def summarize_text(text_element):\n",
    "    prompt = f\"Summarize in detail the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for table summaries\n",
    "def summarize_table(table_element):\n",
    "    prompt = f\"Summarize in detail the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for image summaries\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are an agent skilled at analyzing images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the contents of this image from a patent application\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = chain_gpt_4_vision.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define retreival algorithm and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Initialize the retriever\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to add documents to retrieval algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add documents to the retriever\n",
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    id_key = \"doc_id\"\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding PDF information into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patents = ['US10196086', 'US20180193694A1', 'us10745814', 'us2021229059']\n",
    "\n",
    "for patent in Patents:\n",
    "\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=os.path.join(input_path, f\"{patent}.pdf\"),\n",
    "        extract_images_in_pdf=True, # Get images \n",
    "        infer_table_structure=True, # Get tables \n",
    "        chunking_strategy=\"by_title\", # Preserves sections (by headings and subheadings, as well as over pages where possible)\n",
    "        max_characters=3000, # Define chunk size\n",
    "        new_after_n_chars=1900, # Soft max chunk size \n",
    "        extract_image_block_types=['Image'],\n",
    "        combine_text_under_n_chars=1000, # Min length of chunk size\n",
    "        extract_image_block_output_dir=f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images') #Directory to store images\n",
    "\n",
    "    # Save table and text elements to list\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    image_elements = []\n",
    "\n",
    "    for element in raw_pdf_elements:\n",
    "        if 'CompositeElement' in str(type(element)):\n",
    "            text_elements.append(element)\n",
    "        elif 'Table' in str(type(element)):\n",
    "            table_elements.append(element)\n",
    "    \n",
    "    patent_id = f'###_### {patent}' \n",
    "\n",
    "    table_elements = [i.text for i in table_elements]\n",
    "    table_elements = [''.join([i, patent_id]) for i in table_elements]\n",
    "\n",
    "    text_elements = [i.text for i in text_elements]\n",
    "    text_elements = [''.join([i, patent_id]) for i in text_elements]\n",
    "\n",
    "    # Save image elements to list, converting to format compatible with Vision transformer model\n",
    "\n",
    "    for image_file in os.listdir(f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images'):\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(f'C:/Users/eeo21/VSCodeProjects/PatentRAGApplication/{patent}_images', image_file)\n",
    "            encoded_image = encode_image(image_path)\n",
    "            image_elements.append(encoded_image)\n",
    "\n",
    "    # # Generate Table summaries\n",
    "    table_summaries = []\n",
    "    \n",
    "    for i, te in enumerate(table_elements):\n",
    "        summary = summarize_table(te)\n",
    "        summary = ''.join([summary, patent_id])\n",
    "        table_summaries.append(summary)\n",
    "        print(f\"{i + 1}th element of tables processed.\")\n",
    "\n",
    "    # # Generate Text element summaries\n",
    "    text_summaries = []\n",
    "    for i, te in enumerate(text_elements):\n",
    "        summary = summarize_text(te)\n",
    "        summary = ''.join([summary, patent_id])\n",
    "        text_summaries.append(summary)\n",
    "        print(f\"{i + 1}th element of texts processed.\")\n",
    "\n",
    "    # Generate summaries of images\n",
    "    image_summaries = []\n",
    "    for i, ie in enumerate(image_elements):\n",
    "        try:\n",
    "            summary = summarize_image(ie)\n",
    "            summary = ''.join([summary, patent_id])\n",
    "            image_summaries.append(summary)\n",
    "        except:\n",
    "            print(f'{ie} could not be summarised, could be too large')\n",
    "        print(f\"{i + 1}th element of images processed.\")\n",
    "\n",
    "    # Add text, table and image summaries to vector database\n",
    "    add_documents_to_retriever(text_summaries, text_elements)\n",
    "    add_documents_to_retriever(table_summaries, table_elements)\n",
    "    add_documents_to_retriever(image_summaries, image_summaries) \n",
    "\n",
    "    # Save pdf elements to databases \n",
    "\n",
    "    texts =list(zip(text_summaries, text_elements))\n",
    "    tables = list(zip(table_summaries, table_elements))\n",
    "    images = list(zip(image_summaries, image_summaries))\n",
    "\n",
    "    components = [images]\n",
    "    \n",
    "    PDFdictionary = {}\n",
    "    for component in components:\n",
    "        for pair in component:\n",
    "            PDFdictionary[pair[0]] = [pair[1]]\n",
    "    \n",
    "    with open(f\"{patent}_images.json\", \"w\") as outfile: \n",
    "        json.dump(PDFdictionary, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define inference model and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define prompt template\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text, images and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "Aim = 'Quality' #Change to slow for faster response, perhaps with lower quality\n",
    "\n",
    "if Aim == 'Quality':\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "elif Aim == 'Fast':\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(input):\n",
    "    response = chain.invoke(f'{input}')\n",
    "\n",
    "    contextlist = retriever.get_relevant_documents(f'{input}')\n",
    "\n",
    "    #Remove any duplicated contexts\n",
    "    contextlist = list(dict.fromkeys(contextlist))\n",
    "\n",
    "    #Get ID of patent that context was retreived from \n",
    "    for x in contextlist:\n",
    "        comps = x.split('###_###')\n",
    "        context = comps[0]\n",
    "        ID = comps[1]\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    # Prettify response\n",
    "    response = response.replace('**', '')\n",
    "    response = '\\n'.join([response, f\"{'#'*120} \\n\"])\n",
    "    response = '\\n'.join([response, f\"CONTEXT USED TO GENERATE RESPONSE:\\n {context}\"])\n",
    "    response = '\\n'.join([response, f\"{'#'*120} \\n\"])\n",
    "    response = '\\n'.join([response, f\"CONTEXT FOUND IN PATENT NO. {ID}\"])\n",
    "   \n",
    "    return response\n",
    "\n",
    "inputs = gradio.Textbox(lines=7, label=\"Ask question based on facts from a patent\")\n",
    "outputs = gradio.Textbox(label=\"Reply\")\n",
    "\n",
    "gradio.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"Patent RAG Prototype\",\n",
    "             theme=\"compact\").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
